{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XYaiuqZ1R34",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "In homework assignment 2, we will implement a basic deep Q learning (DQL) algorithm to solve a classic control problem--CartPole V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb-TSGU816Gk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Install the gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "abEdBl_00ggY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from gymnasium) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from gymnasium) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YD3C_WMpC0q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load tensorboard for visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683571808819,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "MTv7U2CInzwq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI4gae3Y2ADK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import the required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16082,
     "status": "ok",
     "timestamp": 1683571827632,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "vxIeU-HZwp_x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\UCR_Projects\\RLProject\\RLenv\\lib\\site-packages\\torch\\__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from typing import Tuple\n",
    "from numpy.random import binomial\n",
    "from numpy.random import choice\n",
    "import numpy.random as nr\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Tensor = torch.DoubleTensor\n",
    "torch.set_default_tensor_type(Tensor)\n",
    "Transitions = namedtuple('Transitions', ['obs', 'action', 'reward', 'next_obs', 'done'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVJcJ-IM2L70",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Replay buffer to collect transition tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683571831530,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "m33LVAxM0BBG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, config):\n",
    "        replay_buffer_size = config['replay_buffer_size']\n",
    "        seed = config['seed']\n",
    "        nr.seed(seed)\n",
    "\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.action = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.reward = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.next_obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.done = deque([], maxlen=self.replay_buffer_size)\n",
    "\n",
    "    def append_memory(self,\n",
    "                      obs,\n",
    "                      action,\n",
    "                      reward,\n",
    "                      next_obs,\n",
    "                      done: bool):\n",
    "        self.obs.append(obs)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.next_obs.append(next_obs)\n",
    "        self.done.append(done)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.obs)\n",
    "\n",
    "        idx = nr.choice(buffer_size,\n",
    "                        size=min(buffer_size, batch_size),\n",
    "                        replace=False)\n",
    "        t = Transitions\n",
    "        t.obs = torch.stack(list(map(self.obs.__getitem__, idx)))\n",
    "        t.action = torch.stack(list(map(self.action.__getitem__, idx)))\n",
    "        t.reward = torch.stack(list(map(self.reward.__getitem__, idx)))\n",
    "        t.next_obs = torch.stack(list(map(self.next_obs.__getitem__, idx)))\n",
    "        t.done = torch.tensor(list(map(self.done.__getitem__, idx)))[:, None]\n",
    "        return t\n",
    "\n",
    "    def clear(self):\n",
    "        self.obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.action = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.reward = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.next_obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.done = deque([], maxlen=self.replay_buffer_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8fWBjcA20jD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683571834223,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "3Mo2I5L50pqY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_obs: int,\n",
    "                 dim_action: int,\n",
    "                 dims_hidden_neurons: Tuple[int] = (64, 64)):\n",
    "        if not isinstance(dim_obs, int):\n",
    "            TypeError('dimension of observation must be int')\n",
    "        if not isinstance(dim_action, int):\n",
    "            TypeError('dimension of action must be int')\n",
    "        if not isinstance(dims_hidden_neurons, tuple):\n",
    "            TypeError('dimensions of hidden neurons must be tuple of int')\n",
    "\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.num_layers = len(dims_hidden_neurons)\n",
    "        self.dim_action = dim_action\n",
    "\n",
    "        n_neurons = (dim_obs, ) + dims_hidden_neurons + (dim_action, )\n",
    "        for i, (dim_in, dim_out) in enumerate(zip(n_neurons[:-2], n_neurons[1:-1])):\n",
    "            layer = nn.Linear(dim_in, dim_out).double()\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            torch.nn.init.zeros_(layer.bias)\n",
    "            exec('self.layer{} = layer'.format(i + 1))\n",
    "\n",
    "        self.output = nn.Linear(n_neurons[-2], n_neurons[-1]).double()\n",
    "        torch.nn.init.xavier_uniform_(self.output.weight)\n",
    "        torch.nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def forward(self, observation: torch.Tensor):\n",
    "        x = observation.double()\n",
    "        for i in range(self.num_layers):\n",
    "            x = eval('torch.tanh(self.layer{}(x))'.format(i + 1))\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIBRWxqp2UYE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DQN agent\n",
    "The base code are given in this section. The updates of the neural networks are missing and are left out for you to fill. You may refer to the DQN papaer: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionality of the `update` Method\n",
    "\n",
    "The `update` method begins by sampling a batch of experiences from the replay buffer. These experiences consist of tuples containing the state (`s`), action (`a`), reward (`r`), next state (`sp`), and done flag (`done`) which indicates whether the episode has terminated. Sampling from a replay buffer instead of using consecutive samples is crucial as it helps to break the correlation between successive training samples, thereby stabilizing the learning process.\n",
    "\n",
    "Once a batch is sampled, the method proceeds with the core of the Q-learning algorithm. It computes the Q-values for the current states (`s`) and the actions taken (`a`). These Q-values are derived from the primary Q-network (`self.Q`). Simultaneously, it calculates the target Q-values, which are used as the \"ground truth\" for training the Q-network. The target Q-values are a combination of the reward received for taking the action (`r`) and the discounted maximum Q-value of the next state (`sp`), obtained from the target Q-network (`self.Q_tar`). This target network's parameters are kept frozen most of the time to provide stable targets, and it's updated from the primary network every `C` steps, ensuring that the learning targets do not fluctuate too drastically.\n",
    "\n",
    "### Loss Calculation and Network Optimization\n",
    "\n",
    "The difference between the Q-values predicted by the primary network and the calculated target Q-values forms the basis for the loss calculation, typically using Mean Squared Error (MSE) as the loss function. This loss reflects the accuracy of the Q-network's predictions and its ability to estimate future rewards based on its current policy.\n",
    "\n",
    "Following the loss calculation, backpropagation is used to update the weights of the primary Q-network, thereby improving its predictions. The optimizer's gradients are first reset to prevent accumulation from previous updates, and then the loss is backpropagated to adjust the network weights in a direction that minimizes the loss. \n",
    "\n",
    "### Synchronization of Primary and Target Networks\n",
    "\n",
    "An essential aspect of this update function is the periodic update of the target Q-network (`self.Q_tar`). This update happens every `C` steps and involves directly copying the weights from the primary Q-network to the target Q-network. This periodic update ensures that the target network slowly tracks the primary network, providing slightly delayed, stable target values for training, which is key to the convergence and stability of the learning algorithm.\n",
    "\n",
    "In summary, the `update` function is a critical method in reinforcement learning that handles the experience replay, calculates and minimizes the loss for the Q-network, and synchronizes the primary and target Q-networks, all of which are pivotal for effective learning and policy improvement in Q-learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1683571847121,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "6bLWOSs90KVp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, config):\n",
    "\n",
    "        torch.manual_seed(config['seed'])\n",
    "\n",
    "        self.lr = config['lr']  # learning rate\n",
    "        self.C = config['C']  # copy steps\n",
    "        self.eps_len = config['eps_len']  # length of epsilon greedy exploration\n",
    "        self.eps_max = config['eps_max']\n",
    "        self.eps_min = config['eps_min']\n",
    "        self.discount = config['discount']  # discount factor\n",
    "        self.batch_size = config['batch_size']  # mini batch size\n",
    "\n",
    "        self.dims_hidden_neurons = config['dims_hidden_neurons']\n",
    "        self.dim_obs = config['dim_obs']\n",
    "        self.dim_action = config['dim_action']\n",
    "\n",
    "        self.Q = QNetwork(dim_obs=self.dim_obs,\n",
    "                          dim_action=self.dim_action,\n",
    "                          dims_hidden_neurons=self.dims_hidden_neurons)\n",
    "        self.Q_tar = QNetwork(dim_obs=self.dim_obs,\n",
    "                              dim_action=self.dim_action,\n",
    "                              dims_hidden_neurons=self.dims_hidden_neurons)\n",
    "\n",
    "        self.optimizer_Q = torch.optim.Adam(self.Q.parameters(), lr=self.lr)\n",
    "        self.training_step = 0\n",
    "\n",
    "    def update(self, buffer):\n",
    "        t = buffer.sample(self.batch_size)\n",
    "\n",
    "        s = t.obs\n",
    "        a = t.action\n",
    "        r = t.reward\n",
    "        sp = t.next_obs\n",
    "        done = t.done\n",
    "\n",
    "        self.training_step += 1\n",
    "\n",
    "        # TODO: perform a single Q network update step. Also update the target Q network every C Q network update steps\n",
    "        # Q network update step\n",
    "        # self.Q.train()\n",
    "        # self.Q_tar.eval()\n",
    "\n",
    "        a = a.long()\n",
    "        # Calculate Q values for current states\n",
    "        Q_current_value = self.Q(s).gather(1, a)\n",
    "    \n",
    "        # Compute target Q values\n",
    "         \n",
    "        # Q_target_value = r + self.discount * torch.max(self.Q_tar(sp), dim=1)[0]\n",
    "        Q_target_value = r + self.discount * self.Q_tar(sp).detach().max(1)[0].unsqueeze(1) * (1 - done.float())\n",
    "\n",
    "        # print(a)\n",
    "        # print(Q_current_value)\n",
    "        # print(self.Q_tar(sp))\n",
    "        # print(Q_target_value)\n",
    "        # print()\n",
    "        # print()\n",
    "        # # Calculate loss\n",
    "        loss = F.mse_loss(Q_current_value, Q_target_value)\n",
    "    \n",
    "        # Optimize the Q network\n",
    "        self.optimizer_Q.zero_grad()  # clear gradients since PyTorch accumulates them\n",
    "        loss.backward()\n",
    "        self.optimizer_Q.step()\n",
    "    \n",
    "        # # Update target Q network every C steps\n",
    "        if self.training_step % self.C == 0:\n",
    "            self.Q_tar.load_state_dict(self.Q.state_dict())\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "\n",
    "    def act_probabilistic(self, observation: torch.Tensor):\n",
    "        # epsilon greedy:\n",
    "        first_term = self.eps_max * (self.eps_len - self.training_step) / self.eps_len\n",
    "        eps = max(first_term, self.eps_min)\n",
    "\n",
    "        explore = binomial(1, eps)\n",
    "\n",
    "        if explore == 1:\n",
    "            a = choice(self.dim_action)\n",
    "        else:\n",
    "            self.Q.eval()\n",
    "            Q = self.Q(observation)\n",
    "            val, a = torch.max(Q, axis=1)\n",
    "            a = a.item()\n",
    "            self.Q.train()\n",
    "        return a\n",
    "\n",
    "    def act_deterministic(self, observation: torch.Tensor):\n",
    "        self.Q.eval()\n",
    "        Q = self.Q(observation)\n",
    "        val, a = torch.max(Q, axis=1)\n",
    "        self.Q.train()\n",
    "        return a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbjagrA3pXS7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1683571851068,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "zAgJklHgzOmg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "config = {\n",
    "    'dim_obs': 4,  # Q network input\n",
    "    'dim_action': 2,  # Q network output\n",
    "    'dims_hidden_neurons': (64, 64),  # Q network hidden\n",
    "    'lr': 0.0001,  # learning rate\n",
    "    'C': 20,  # copy steps\n",
    "    'discount': 0.99,  # discount factor\n",
    "    'batch_size': 64,\n",
    "    'replay_buffer_size': 100000,\n",
    "    'eps_min': 0.01,\n",
    "    'eps_max': 1.0,\n",
    "    'eps_len': 4000,\n",
    "    'seed': 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVuWeg79pdk-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683571853631,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "MAKp2pKgzYZM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dqn = DQN(config)\n",
    "buffer = ReplayBuffer(config)\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "log_dir = f'tensorboard/dqn/data_{current_time}_lr-{config[\"lr\"]}_batch_size-{config[\"batch_size\"]}_C-{config[\"C\"]}_seed-{config[\"seed\"]}'\n",
    "train_writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9MJiJNwphUe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "0kfeRfmDzRk5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 15 timesteps\n",
      "Episode 1 finished after 89 timesteps\n",
      "Episode 2 finished after 12 timesteps\n",
      "Episode 3 finished after 16 timesteps\n",
      "Episode 4 finished after 28 timesteps\n",
      "Episode 5 finished after 15 timesteps\n",
      "Episode 6 finished after 24 timesteps\n",
      "Episode 7 finished after 19 timesteps\n",
      "Episode 8 finished after 19 timesteps\n",
      "Episode 9 finished after 29 timesteps\n",
      "Episode 10 finished after 38 timesteps\n",
      "Episode 11 finished after 23 timesteps\n",
      "Episode 12 finished after 24 timesteps\n",
      "Episode 13 finished after 13 timesteps\n",
      "Episode 14 finished after 14 timesteps\n",
      "Episode 15 finished after 22 timesteps\n",
      "Episode 16 finished after 11 timesteps\n",
      "Episode 17 finished after 12 timesteps\n",
      "Episode 18 finished after 39 timesteps\n",
      "Episode 19 finished after 16 timesteps\n",
      "Episode 20 finished after 13 timesteps\n",
      "Episode 21 finished after 21 timesteps\n",
      "Episode 22 finished after 18 timesteps\n",
      "Episode 23 finished after 17 timesteps\n",
      "Episode 24 finished after 23 timesteps\n",
      "Episode 25 finished after 30 timesteps\n",
      "Episode 26 finished after 17 timesteps\n",
      "Episode 27 finished after 14 timesteps\n",
      "Episode 28 finished after 16 timesteps\n",
      "Episode 29 finished after 27 timesteps\n",
      "Episode 30 finished after 24 timesteps\n",
      "Episode 31 finished after 29 timesteps\n",
      "Episode 32 finished after 16 timesteps\n",
      "Episode 33 finished after 11 timesteps\n",
      "Episode 34 finished after 46 timesteps\n",
      "Episode 35 finished after 18 timesteps\n",
      "Episode 36 finished after 14 timesteps\n",
      "Episode 37 finished after 44 timesteps\n",
      "Episode 38 finished after 54 timesteps\n",
      "Episode 39 finished after 13 timesteps\n",
      "Episode 40 finished after 25 timesteps\n",
      "Episode 41 finished after 64 timesteps\n",
      "Episode 42 finished after 58 timesteps\n",
      "Episode 43 finished after 26 timesteps\n",
      "Episode 44 finished after 24 timesteps\n",
      "Episode 45 finished after 82 timesteps\n",
      "Episode 46 finished after 25 timesteps\n",
      "Episode 47 finished after 22 timesteps\n",
      "Episode 48 finished after 18 timesteps\n",
      "Episode 49 finished after 16 timesteps\n",
      "Episode 50 finished after 13 timesteps\n",
      "Episode 51 finished after 13 timesteps\n",
      "Episode 52 finished after 14 timesteps\n",
      "Episode 53 finished after 16 timesteps\n",
      "Episode 54 finished after 14 timesteps\n",
      "Episode 55 finished after 21 timesteps\n",
      "Episode 56 finished after 38 timesteps\n",
      "Episode 57 finished after 25 timesteps\n",
      "Episode 58 finished after 20 timesteps\n",
      "Episode 59 finished after 33 timesteps\n",
      "Episode 60 finished after 27 timesteps\n",
      "Episode 61 finished after 17 timesteps\n",
      "Episode 62 finished after 13 timesteps\n",
      "Episode 63 finished after 21 timesteps\n",
      "Episode 64 finished after 14 timesteps\n",
      "Episode 65 finished after 10 timesteps\n",
      "Episode 66 finished after 12 timesteps\n",
      "Episode 67 finished after 11 timesteps\n",
      "Episode 68 finished after 11 timesteps\n",
      "Episode 69 finished after 13 timesteps\n",
      "Episode 70 finished after 20 timesteps\n",
      "Episode 71 finished after 25 timesteps\n",
      "Episode 72 finished after 27 timesteps\n",
      "Episode 73 finished after 11 timesteps\n",
      "Episode 74 finished after 14 timesteps\n",
      "Episode 75 finished after 13 timesteps\n",
      "Episode 76 finished after 12 timesteps\n",
      "Episode 77 finished after 15 timesteps\n",
      "Episode 78 finished after 13 timesteps\n",
      "Episode 79 finished after 13 timesteps\n",
      "Episode 80 finished after 11 timesteps\n",
      "Episode 81 finished after 13 timesteps\n",
      "Episode 82 finished after 18 timesteps\n",
      "Episode 83 finished after 35 timesteps\n",
      "Episode 84 finished after 9 timesteps\n",
      "Episode 85 finished after 13 timesteps\n",
      "Episode 86 finished after 13 timesteps\n",
      "Episode 87 finished after 18 timesteps\n",
      "Episode 88 finished after 25 timesteps\n",
      "Episode 89 finished after 18 timesteps\n",
      "Episode 90 finished after 22 timesteps\n",
      "Episode 91 finished after 21 timesteps\n",
      "Episode 92 finished after 22 timesteps\n",
      "Episode 93 finished after 20 timesteps\n",
      "Episode 94 finished after 46 timesteps\n",
      "Episode 95 finished after 25 timesteps\n",
      "Episode 96 finished after 115 timesteps\n",
      "Episode 97 finished after 94 timesteps\n",
      "Episode 98 finished after 29 timesteps\n",
      "Episode 99 finished after 66 timesteps\n",
      "Episode 100 finished after 31 timesteps\n",
      "Episode 101 finished after 129 timesteps\n",
      "Episode 102 finished after 52 timesteps\n",
      "Episode 103 finished after 69 timesteps\n",
      "Episode 104 finished after 123 timesteps\n",
      "Episode 105 finished after 103 timesteps\n",
      "Episode 106 finished after 105 timesteps\n",
      "Episode 107 finished after 91 timesteps\n",
      "Episode 108 finished after 114 timesteps\n",
      "Episode 109 finished after 56 timesteps\n",
      "Episode 110 finished after 55 timesteps\n",
      "Episode 111 finished after 263 timesteps\n",
      "Episode 112 finished after 121 timesteps\n",
      "Episode 113 finished after 63 timesteps\n",
      "Episode 114 finished after 61 timesteps\n",
      "Episode 115 finished after 75 timesteps\n",
      "Episode 116 finished after 65 timesteps\n",
      "Episode 117 finished after 75 timesteps\n",
      "Episode 118 finished after 157 timesteps\n",
      "Episode 119 finished after 73 timesteps\n",
      "Episode 120 finished after 58 timesteps\n",
      "Episode 121 finished after 60 timesteps\n",
      "Episode 122 finished after 76 timesteps\n",
      "Episode 123 finished after 67 timesteps\n",
      "Episode 124 finished after 93 timesteps\n",
      "Episode 125 finished after 151 timesteps\n",
      "Episode 126 finished after 204 timesteps\n",
      "Episode 127 finished after 101 timesteps\n",
      "Episode 128 finished after 101 timesteps\n",
      "Episode 129 finished after 203 timesteps\n",
      "Episode 130 finished after 165 timesteps\n",
      "Episode 131 finished after 396 timesteps\n",
      "Episode 132 finished after 65 timesteps\n",
      "Episode 133 finished after 54 timesteps\n",
      "Episode 134 finished after 152 timesteps\n",
      "Episode 135 finished after 157 timesteps\n",
      "Episode 136 finished after 212 timesteps\n",
      "Episode 137 finished after 73 timesteps\n",
      "Episode 138 finished after 68 timesteps\n",
      "Episode 139 finished after 71 timesteps\n",
      "Episode 140 finished after 170 timesteps\n",
      "Episode 141 finished after 68 timesteps\n",
      "Episode 142 finished after 83 timesteps\n",
      "Episode 143 finished after 96 timesteps\n",
      "Episode 144 finished after 133 timesteps\n",
      "Episode 145 finished after 193 timesteps\n",
      "Episode 146 finished after 184 timesteps\n",
      "Episode 147 finished after 107 timesteps\n",
      "Episode 148 finished after 187 timesteps\n",
      "Episode 149 finished after 317 timesteps\n",
      "Episode 150 finished after 142 timesteps\n",
      "Episode 151 finished after 309 timesteps\n",
      "Episode 152 finished after 254 timesteps\n",
      "Episode 153 finished after 461 timesteps\n",
      "Episode 154 finished after 248 timesteps\n",
      "Episode 155 finished after 311 timesteps\n",
      "Episode 156 finished after 306 timesteps\n",
      "Episode 157 finished after 255 timesteps\n",
      "Episode 158 finished after 258 timesteps\n",
      "Episode 159 finished after 248 timesteps\n",
      "Episode 160 finished after 261 timesteps\n",
      "Episode 161 finished after 238 timesteps\n",
      "Episode 162 finished after 220 timesteps\n",
      "Episode 163 finished after 223 timesteps\n",
      "Episode 164 finished after 264 timesteps\n",
      "Episode 165 finished after 284 timesteps\n",
      "Episode 166 finished after 274 timesteps\n",
      "Episode 167 finished after 395 timesteps\n",
      "Episode 168 finished after 254 timesteps\n",
      "Episode 169 finished after 341 timesteps\n",
      "Episode 170 finished after 382 timesteps\n",
      "Episode 171 finished after 323 timesteps\n",
      "Episode 172 finished after 337 timesteps\n",
      "Episode 173 finished after 369 timesteps\n",
      "Episode 174 finished after 332 timesteps\n",
      "Episode 175 finished after 501 timesteps\n",
      "Episode 176 finished after 455 timesteps\n",
      "Episode 177 finished after 237 timesteps\n",
      "Episode 178 finished after 323 timesteps\n",
      "Episode 179 finished after 501 timesteps\n",
      "Episode 180 finished after 236 timesteps\n",
      "Episode 181 finished after 223 timesteps\n",
      "Episode 182 finished after 371 timesteps\n",
      "Episode 183 finished after 456 timesteps\n",
      "Episode 184 finished after 314 timesteps\n",
      "Episode 185 finished after 469 timesteps\n",
      "Episode 186 finished after 309 timesteps\n",
      "Episode 187 finished after 501 timesteps\n",
      "Episode 188 finished after 427 timesteps\n",
      "Episode 189 finished after 501 timesteps\n",
      "Episode 190 finished after 353 timesteps\n",
      "Episode 191 finished after 427 timesteps\n",
      "Episode 192 finished after 501 timesteps\n",
      "Episode 193 finished after 324 timesteps\n",
      "Episode 194 finished after 304 timesteps\n",
      "Episode 195 finished after 433 timesteps\n",
      "Episode 196 finished after 501 timesteps\n",
      "Episode 197 finished after 315 timesteps\n",
      "Episode 198 finished after 501 timesteps\n",
      "Episode 199 finished after 449 timesteps\n",
      "Episode 200 finished after 501 timesteps\n",
      "Episode 201 finished after 501 timesteps\n",
      "Episode 202 finished after 491 timesteps\n",
      "Episode 203 finished after 501 timesteps\n",
      "Episode 204 finished after 411 timesteps\n",
      "Episode 205 finished after 501 timesteps\n",
      "Episode 206 finished after 501 timesteps\n",
      "Episode 207 finished after 327 timesteps\n",
      "Episode 208 finished after 474 timesteps\n",
      "Episode 209 finished after 433 timesteps\n",
      "Episode 210 finished after 441 timesteps\n",
      "Episode 211 finished after 306 timesteps\n",
      "Episode 212 finished after 303 timesteps\n",
      "Episode 213 finished after 256 timesteps\n",
      "Episode 214 finished after 291 timesteps\n",
      "Episode 215 finished after 292 timesteps\n",
      "Episode 216 finished after 324 timesteps\n",
      "Episode 217 finished after 268 timesteps\n",
      "Episode 218 finished after 235 timesteps\n",
      "Episode 219 finished after 226 timesteps\n",
      "Episode 220 finished after 264 timesteps\n",
      "Episode 221 finished after 213 timesteps\n",
      "Episode 222 finished after 229 timesteps\n",
      "Episode 223 finished after 176 timesteps\n",
      "Episode 224 finished after 207 timesteps\n",
      "Episode 225 finished after 201 timesteps\n",
      "Episode 226 finished after 206 timesteps\n",
      "Episode 227 finished after 202 timesteps\n",
      "Episode 228 finished after 200 timesteps\n",
      "Episode 229 finished after 206 timesteps\n",
      "Episode 230 finished after 227 timesteps\n",
      "Episode 231 finished after 213 timesteps\n",
      "Episode 232 finished after 205 timesteps\n",
      "Episode 233 finished after 205 timesteps\n",
      "Episode 234 finished after 220 timesteps\n",
      "Episode 235 finished after 229 timesteps\n",
      "Episode 236 finished after 200 timesteps\n",
      "Episode 237 finished after 196 timesteps\n",
      "Episode 238 finished after 201 timesteps\n",
      "Episode 239 finished after 205 timesteps\n",
      "Episode 240 finished after 219 timesteps\n",
      "Episode 241 finished after 227 timesteps\n",
      "Episode 242 finished after 180 timesteps\n",
      "Episode 243 finished after 217 timesteps\n",
      "Episode 244 finished after 227 timesteps\n",
      "Episode 245 finished after 212 timesteps\n",
      "Episode 246 finished after 182 timesteps\n",
      "Episode 247 finished after 234 timesteps\n",
      "Episode 248 finished after 192 timesteps\n",
      "Episode 249 finished after 206 timesteps\n",
      "Episode 250 finished after 188 timesteps\n",
      "Episode 251 finished after 205 timesteps\n",
      "Episode 252 finished after 207 timesteps\n",
      "Episode 253 finished after 179 timesteps\n",
      "Episode 254 finished after 218 timesteps\n",
      "Episode 255 finished after 198 timesteps\n",
      "Episode 256 finished after 193 timesteps\n",
      "Episode 257 finished after 185 timesteps\n",
      "Episode 258 finished after 176 timesteps\n",
      "Episode 259 finished after 194 timesteps\n",
      "Episode 260 finished after 200 timesteps\n",
      "Episode 261 finished after 194 timesteps\n",
      "Episode 262 finished after 216 timesteps\n",
      "Episode 263 finished after 220 timesteps\n",
      "Episode 264 finished after 208 timesteps\n",
      "Episode 265 finished after 177 timesteps\n",
      "Episode 266 finished after 203 timesteps\n",
      "Episode 267 finished after 246 timesteps\n",
      "Episode 268 finished after 187 timesteps\n",
      "Episode 269 finished after 177 timesteps\n",
      "Episode 270 finished after 176 timesteps\n",
      "Episode 271 finished after 186 timesteps\n",
      "Episode 272 finished after 180 timesteps\n",
      "Episode 273 finished after 167 timesteps\n",
      "Episode 274 finished after 181 timesteps\n",
      "Episode 275 finished after 187 timesteps\n",
      "Episode 276 finished after 175 timesteps\n",
      "Episode 277 finished after 194 timesteps\n",
      "Episode 278 finished after 171 timesteps\n",
      "Episode 279 finished after 164 timesteps\n",
      "Episode 280 finished after 240 timesteps\n",
      "Episode 281 finished after 294 timesteps\n",
      "Episode 282 finished after 172 timesteps\n",
      "Episode 283 finished after 227 timesteps\n",
      "Episode 284 finished after 182 timesteps\n",
      "Episode 285 finished after 187 timesteps\n",
      "Episode 286 finished after 222 timesteps\n",
      "Episode 287 finished after 231 timesteps\n",
      "Episode 288 finished after 196 timesteps\n",
      "Episode 289 finished after 227 timesteps\n",
      "Episode 290 finished after 404 timesteps\n",
      "Episode 291 finished after 324 timesteps\n",
      "Episode 292 finished after 367 timesteps\n",
      "Episode 293 finished after 265 timesteps\n",
      "Episode 294 finished after 501 timesteps\n",
      "Episode 295 finished after 501 timesteps\n",
      "Episode 296 finished after 196 timesteps\n",
      "Episode 297 finished after 501 timesteps\n",
      "Episode 298 finished after 309 timesteps\n",
      "Episode 299 finished after 414 timesteps\n",
      "Episode 300 finished after 445 timesteps\n",
      "Episode 301 finished after 347 timesteps\n",
      "Episode 302 finished after 376 timesteps\n",
      "Episode 303 finished after 501 timesteps\n",
      "Episode 304 finished after 501 timesteps\n",
      "Episode 305 finished after 501 timesteps\n",
      "Episode 306 finished after 453 timesteps\n",
      "Episode 307 finished after 501 timesteps\n",
      "Episode 308 finished after 501 timesteps\n",
      "Episode 309 finished after 286 timesteps\n",
      "Episode 310 finished after 501 timesteps\n",
      "Episode 311 finished after 218 timesteps\n",
      "Episode 312 finished after 275 timesteps\n",
      "Episode 313 finished after 215 timesteps\n",
      "Episode 314 finished after 176 timesteps\n",
      "Episode 315 finished after 364 timesteps\n",
      "Episode 316 finished after 501 timesteps\n",
      "Episode 317 finished after 501 timesteps\n",
      "Episode 318 finished after 469 timesteps\n",
      "Episode 319 finished after 501 timesteps\n",
      "Episode 320 finished after 501 timesteps\n",
      "Episode 321 finished after 501 timesteps\n",
      "Episode 322 finished after 218 timesteps\n",
      "Episode 323 finished after 229 timesteps\n",
      "Episode 324 finished after 501 timesteps\n",
      "Episode 325 finished after 444 timesteps\n",
      "Episode 326 finished after 330 timesteps\n",
      "Episode 327 finished after 501 timesteps\n",
      "Episode 328 finished after 501 timesteps\n",
      "Episode 329 finished after 203 timesteps\n",
      "Episode 330 finished after 281 timesteps\n",
      "Episode 331 finished after 351 timesteps\n",
      "Episode 332 finished after 420 timesteps\n",
      "Episode 333 finished after 183 timesteps\n",
      "Episode 334 finished after 501 timesteps\n",
      "Episode 335 finished after 377 timesteps\n",
      "Episode 336 finished after 223 timesteps\n",
      "Episode 337 finished after 174 timesteps\n",
      "Episode 338 finished after 180 timesteps\n",
      "Episode 339 finished after 206 timesteps\n",
      "Episode 340 finished after 180 timesteps\n",
      "Episode 341 finished after 240 timesteps\n",
      "Episode 342 finished after 208 timesteps\n",
      "Episode 343 finished after 202 timesteps\n",
      "Episode 344 finished after 441 timesteps\n",
      "Episode 345 finished after 195 timesteps\n",
      "Episode 346 finished after 189 timesteps\n",
      "Episode 347 finished after 171 timesteps\n",
      "Episode 348 finished after 204 timesteps\n",
      "Episode 349 finished after 214 timesteps\n",
      "Episode 350 finished after 213 timesteps\n",
      "Episode 351 finished after 201 timesteps\n",
      "Episode 352 finished after 228 timesteps\n",
      "Episode 353 finished after 186 timesteps\n",
      "Episode 354 finished after 189 timesteps\n",
      "Episode 355 finished after 205 timesteps\n",
      "Episode 356 finished after 207 timesteps\n",
      "Episode 357 finished after 205 timesteps\n",
      "Episode 358 finished after 188 timesteps\n",
      "Episode 359 finished after 174 timesteps\n",
      "Episode 360 finished after 192 timesteps\n",
      "Episode 361 finished after 203 timesteps\n",
      "Episode 362 finished after 191 timesteps\n",
      "Episode 363 finished after 218 timesteps\n",
      "Episode 364 finished after 236 timesteps\n",
      "Episode 365 finished after 220 timesteps\n",
      "Episode 366 finished after 194 timesteps\n",
      "Episode 367 finished after 223 timesteps\n",
      "Episode 368 finished after 212 timesteps\n",
      "Episode 369 finished after 273 timesteps\n",
      "Episode 370 finished after 247 timesteps\n",
      "Episode 371 finished after 215 timesteps\n",
      "Episode 372 finished after 276 timesteps\n",
      "Episode 373 finished after 293 timesteps\n",
      "Episode 374 finished after 274 timesteps\n",
      "Episode 375 finished after 321 timesteps\n",
      "Episode 376 finished after 501 timesteps\n",
      "Episode 377 finished after 501 timesteps\n",
      "Episode 378 finished after 501 timesteps\n",
      "Episode 379 finished after 501 timesteps\n",
      "Episode 380 finished after 501 timesteps\n",
      "Episode 381 finished after 501 timesteps\n",
      "Episode 382 finished after 501 timesteps\n",
      "Episode 383 finished after 501 timesteps\n",
      "Episode 384 finished after 501 timesteps\n",
      "Episode 385 finished after 501 timesteps\n",
      "Episode 386 finished after 501 timesteps\n",
      "Episode 387 finished after 501 timesteps\n",
      "Episode 388 finished after 501 timesteps\n",
      "Episode 389 finished after 501 timesteps\n",
      "Episode 390 finished after 501 timesteps\n",
      "Episode 391 finished after 501 timesteps\n",
      "Episode 392 finished after 501 timesteps\n",
      "Episode 393 finished after 501 timesteps\n",
      "Episode 394 finished after 501 timesteps\n",
      "Episode 395 finished after 501 timesteps\n",
      "Episode 396 finished after 435 timesteps\n",
      "Episode 397 finished after 475 timesteps\n",
      "Episode 398 finished after 501 timesteps\n",
      "Episode 399 finished after 469 timesteps\n",
      "Episode 400 finished after 313 timesteps\n",
      "Episode 401 finished after 474 timesteps\n",
      "Episode 402 finished after 326 timesteps\n",
      "Episode 403 finished after 391 timesteps\n",
      "Episode 404 finished after 343 timesteps\n",
      "Episode 405 finished after 471 timesteps\n",
      "Episode 406 finished after 501 timesteps\n",
      "Episode 407 finished after 408 timesteps\n",
      "Episode 408 finished after 423 timesteps\n",
      "Episode 409 finished after 405 timesteps\n",
      "Episode 410 finished after 415 timesteps\n",
      "Episode 411 finished after 478 timesteps\n",
      "Episode 412 finished after 485 timesteps\n",
      "Episode 413 finished after 501 timesteps\n",
      "Episode 414 finished after 375 timesteps\n",
      "Episode 415 finished after 345 timesteps\n",
      "Episode 416 finished after 301 timesteps\n",
      "Episode 417 finished after 501 timesteps\n",
      "Episode 418 finished after 501 timesteps\n",
      "Episode 419 finished after 248 timesteps\n",
      "Episode 420 finished after 349 timesteps\n",
      "Episode 421 finished after 388 timesteps\n",
      "Episode 422 finished after 374 timesteps\n",
      "Episode 423 finished after 252 timesteps\n",
      "Episode 424 finished after 414 timesteps\n",
      "Episode 425 finished after 374 timesteps\n",
      "Episode 426 finished after 380 timesteps\n",
      "Episode 427 finished after 328 timesteps\n",
      "Episode 428 finished after 296 timesteps\n",
      "Episode 429 finished after 296 timesteps\n",
      "Episode 430 finished after 438 timesteps\n",
      "Episode 431 finished after 330 timesteps\n",
      "Episode 432 finished after 334 timesteps\n",
      "Episode 433 finished after 336 timesteps\n",
      "Episode 434 finished after 350 timesteps\n",
      "Episode 435 finished after 242 timesteps\n",
      "Episode 436 finished after 372 timesteps\n",
      "Episode 437 finished after 340 timesteps\n",
      "Episode 438 finished after 354 timesteps\n",
      "Episode 439 finished after 298 timesteps\n",
      "Episode 440 finished after 264 timesteps\n",
      "Episode 441 finished after 309 timesteps\n",
      "Episode 442 finished after 387 timesteps\n",
      "Episode 443 finished after 333 timesteps\n",
      "Episode 444 finished after 337 timesteps\n",
      "Episode 445 finished after 272 timesteps\n",
      "Episode 446 finished after 294 timesteps\n",
      "Episode 447 finished after 261 timesteps\n",
      "Episode 448 finished after 385 timesteps\n",
      "Episode 449 finished after 257 timesteps\n",
      "Episode 450 finished after 310 timesteps\n",
      "Episode 451 finished after 291 timesteps\n",
      "Episode 452 finished after 304 timesteps\n",
      "Episode 453 finished after 336 timesteps\n",
      "Episode 454 finished after 501 timesteps\n",
      "Episode 455 finished after 492 timesteps\n",
      "Episode 456 finished after 501 timesteps\n",
      "Episode 457 finished after 501 timesteps\n",
      "Episode 458 finished after 501 timesteps\n",
      "Episode 459 finished after 501 timesteps\n",
      "Episode 460 finished after 501 timesteps\n",
      "Episode 461 finished after 501 timesteps\n",
      "Episode 462 finished after 501 timesteps\n",
      "Episode 463 finished after 501 timesteps\n",
      "Episode 464 finished after 501 timesteps\n",
      "Episode 465 finished after 501 timesteps\n",
      "Episode 466 finished after 501 timesteps\n",
      "Episode 467 finished after 501 timesteps\n",
      "Episode 468 finished after 501 timesteps\n",
      "Episode 469 finished after 501 timesteps\n",
      "Episode 470 finished after 501 timesteps\n",
      "Episode 471 finished after 501 timesteps\n",
      "Episode 472 finished after 501 timesteps\n",
      "Episode 473 finished after 501 timesteps\n",
      "Episode 474 finished after 501 timesteps\n",
      "Episode 475 finished after 501 timesteps\n",
      "Episode 476 finished after 501 timesteps\n",
      "Episode 477 finished after 501 timesteps\n",
      "Episode 478 finished after 501 timesteps\n",
      "Episode 479 finished after 501 timesteps\n",
      "Episode 480 finished after 501 timesteps\n",
      "Episode 481 finished after 501 timesteps\n",
      "Episode 482 finished after 501 timesteps\n",
      "Episode 483 finished after 501 timesteps\n",
      "Episode 484 finished after 501 timesteps\n",
      "Episode 485 finished after 501 timesteps\n",
      "Episode 486 finished after 501 timesteps\n",
      "Episode 487 finished after 501 timesteps\n",
      "Episode 488 finished after 501 timesteps\n",
      "Episode 489 finished after 501 timesteps\n",
      "Episode 490 finished after 501 timesteps\n",
      "Episode 491 finished after 501 timesteps\n",
      "Episode 492 finished after 501 timesteps\n",
      "Episode 493 finished after 501 timesteps\n",
      "Episode 494 finished after 501 timesteps\n",
      "Episode 495 finished after 501 timesteps\n",
      "Episode 496 finished after 501 timesteps\n",
      "Episode 497 finished after 501 timesteps\n",
      "Episode 498 finished after 501 timesteps\n",
      "Episode 499 finished after 501 timesteps\n"
     ]
    }
   ],
   "source": [
    "steps = 0  # total number of steps\n",
    "\n",
    "for i_episode in range(500):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    t = 0  # time steps within each episode\n",
    "    ret = 0.  # episodic return\n",
    "    while done is False and truncated is False:\n",
    "        # env.render()  # render to screen, not working for jupyter\n",
    "\n",
    "        obs = torch.tensor(env.state)  # observe the environment state\n",
    "\n",
    "        action = dqn.act_probabilistic(obs[None, :])  # take action\n",
    "\n",
    "        next_obs, reward, done, truncated, info = env.step(action)  # environment advance to next step\n",
    "\n",
    "        buffer.append_memory(obs=obs,  # put the transition to memory\n",
    "                             action=torch.from_numpy(np.array([action])),\n",
    "                             reward=torch.from_numpy(np.array([reward])),\n",
    "                             next_obs=torch.from_numpy(next_obs),\n",
    "                             done=done)\n",
    "\n",
    "        dqn.update(buffer)  # agent learn\n",
    "\n",
    "        t += 1\n",
    "        steps += 1\n",
    "        \n",
    "        # print(f\"Step: {t}, Total Steps: {steps}, Done: {done}, Truncated: {truncated}, Info: {info}\")\n",
    "\n",
    "        ret += reward  # update episodic return\n",
    "        if done or truncated:\n",
    "            print(\"Episode {} finished after {} timesteps\".format(i_episode, t+1))\n",
    "        train_writer.add_scalar('Performance/episodic_return', ret, i_episode)  # plot\n",
    "\n",
    "env.close()\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fONEK3AplX3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ReuJMkvHgcCA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 21176), started 3:38:22 ago. (Use '!kill 21176' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a8880b58f53ce341\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a8880b58f53ce341\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir='tensorboard/dqn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNWQ2OaUIKGRb8LDhnzck9Z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}