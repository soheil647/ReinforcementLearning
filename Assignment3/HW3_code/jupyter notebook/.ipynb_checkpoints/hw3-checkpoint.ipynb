{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNWQ2OaUIKGRb8LDhnzck9Z"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "In homework assignment 2, we will implement a basic deep Q learning (DQL) algorithm to solve a classic control problem--CartPole V1"
   ],
   "metadata": {
    "id": "9XYaiuqZ1R34"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install the gym environment"
   ],
   "metadata": {
    "id": "pb-TSGU816Gk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gymnasium"
   ],
   "metadata": {
    "id": "abEdBl_00ggY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load tensorboard for visualizing"
   ],
   "metadata": {
    "id": "5YD3C_WMpC0q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "id": "MTv7U2CInzwq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1683571808819,
     "user_tz": 420,
     "elapsed": 3,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     }
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import the required package"
   ],
   "metadata": {
    "id": "KI4gae3Y2ADK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vxIeU-HZwp_x",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1683571827632,
     "user_tz": 420,
     "elapsed": 16082,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     }
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from typing import Tuple\n",
    "from numpy.random import binomial\n",
    "from numpy.random import choice\n",
    "import numpy.random as nr\n",
    "\n",
    "Tensor = torch.DoubleTensor\n",
    "torch.set_default_tensor_type(Tensor)\n",
    "Transitions = namedtuple('Transitions', ['obs', 'action', 'reward', 'next_obs', 'done'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Replay buffer to collect transition tuples"
   ],
   "metadata": {
    "id": "RVJcJ-IM2L70"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, config):\n",
    "        replay_buffer_size = config['replay_buffer_size']\n",
    "        seed = config['seed']\n",
    "        nr.seed(seed)\n",
    "\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.action = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.reward = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.next_obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.done = deque([], maxlen=self.replay_buffer_size)\n",
    "\n",
    "    def append_memory(self,\n",
    "                      obs,\n",
    "                      action,\n",
    "                      reward,\n",
    "                      next_obs,\n",
    "                      done: bool):\n",
    "        self.obs.append(obs)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.next_obs.append(next_obs)\n",
    "        self.done.append(done)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.obs)\n",
    "\n",
    "        idx = nr.choice(buffer_size,\n",
    "                        size=min(buffer_size, batch_size),\n",
    "                        replace=False)\n",
    "        t = Transitions\n",
    "        t.obs = torch.stack(list(map(self.obs.__getitem__, idx)))\n",
    "        t.action = torch.stack(list(map(self.action.__getitem__, idx)))\n",
    "        t.reward = torch.stack(list(map(self.reward.__getitem__, idx)))\n",
    "        t.next_obs = torch.stack(list(map(self.next_obs.__getitem__, idx)))\n",
    "        t.done = torch.tensor(list(map(self.done.__getitem__, idx)))[:, None]\n",
    "        return t\n",
    "\n",
    "    def clear(self):\n",
    "        self.obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.action = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.reward = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.next_obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.done = deque([], maxlen=self.replay_buffer_size)\n"
   ],
   "metadata": {
    "id": "m33LVAxM0BBG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1683571831530,
     "user_tz": 420,
     "elapsed": 4,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     }
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q network"
   ],
   "metadata": {
    "id": "k8fWBjcA20jD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_obs: int,\n",
    "                 dim_action: int,\n",
    "                 dims_hidden_neurons: Tuple[int] = (64, 64)):\n",
    "        if not isinstance(dim_obs, int):\n",
    "            TypeError('dimension of observation must be int')\n",
    "        if not isinstance(dim_action, int):\n",
    "            TypeError('dimension of action must be int')\n",
    "        if not isinstance(dims_hidden_neurons, tuple):\n",
    "            TypeError('dimensions of hidden neurons must be tuple of int')\n",
    "\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.num_layers = len(dims_hidden_neurons)\n",
    "        self.dim_action = dim_action\n",
    "\n",
    "        n_neurons = (dim_obs, ) + dims_hidden_neurons + (dim_action, )\n",
    "        for i, (dim_in, dim_out) in enumerate(zip(n_neurons[:-2], n_neurons[1:-1])):\n",
    "            layer = nn.Linear(dim_in, dim_out).double()\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            torch.nn.init.zeros_(layer.bias)\n",
    "            exec('self.layer{} = layer'.format(i + 1))\n",
    "\n",
    "        self.output = nn.Linear(n_neurons[-2], n_neurons[-1]).double()\n",
    "        torch.nn.init.xavier_uniform_(self.output.weight)\n",
    "        torch.nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def forward(self, observation: torch.Tensor):\n",
    "        x = observation.double()\n",
    "        for i in range(self.num_layers):\n",
    "            x = eval('torch.tanh(self.layer{}(x))'.format(i + 1))\n",
    "        return self.output(x)\n"
   ],
   "metadata": {
    "id": "3Mo2I5L50pqY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1683571834223,
     "user_tz": 420,
     "elapsed": 4,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DQN agent\n",
    "The base code are given in this section. The updates of the neural networks are missing and are left out for you to fill. You may refer to the DQN papaer: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ],
   "metadata": {
    "id": "KIBRWxqp2UYE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DQN:\n",
    "    def __init__(self, config):\n",
    "\n",
    "        torch.manual_seed(config['seed'])\n",
    "\n",
    "        self.lr = config['lr']  # learning rate\n",
    "        self.C = config['C']  # copy steps\n",
    "        self.eps_len = config['eps_len']  # length of epsilon greedy exploration\n",
    "        self.eps_max = config['eps_max']\n",
    "        self.eps_min = config['eps_min']\n",
    "        self.discount = config['discount']  # discount factor\n",
    "        self.batch_size = config['batch_size']  # mini batch size\n",
    "\n",
    "        self.dims_hidden_neurons = config['dims_hidden_neurons']\n",
    "        self.dim_obs = config['dim_obs']\n",
    "        self.dim_action = config['dim_action']\n",
    "\n",
    "        self.Q = QNetwork(dim_obs=self.dim_obs,\n",
    "                          dim_action=self.dim_action,\n",
    "                          dims_hidden_neurons=self.dims_hidden_neurons)\n",
    "        self.Q_tar = QNetwork(dim_obs=self.dim_obs,\n",
    "                              dim_action=self.dim_action,\n",
    "                              dims_hidden_neurons=self.dims_hidden_neurons)\n",
    "\n",
    "        self.optimizer_Q = torch.optim.Adam(self.Q.parameters(), lr=self.lr)\n",
    "        self.training_step = 0\n",
    "\n",
    "    def update(self, buffer):\n",
    "        t = buffer.sample(self.batch_size)\n",
    "\n",
    "        s = t.obs\n",
    "        a = t.action\n",
    "        r = t.reward\n",
    "        sp = t.next_obs\n",
    "        done = t.done\n",
    "\n",
    "        self.training_step += 1\n",
    "\n",
    "        # TODO: perform a single Q network update step. Also update the target Q network every C Q network update steps\n",
    "\n",
    "\n",
    "\n",
    "    def act_probabilistic(self, observation: torch.Tensor):\n",
    "        # epsilon greedy:\n",
    "        first_term = self.eps_max * (self.eps_len - self.training_step) / self.eps_len\n",
    "        eps = max(first_term, self.eps_min)\n",
    "\n",
    "        explore = binomial(1, eps)\n",
    "\n",
    "        if explore == 1:\n",
    "            a = choice(self.dim_action)\n",
    "        else:\n",
    "            self.Q.eval()\n",
    "            Q = self.Q(observation)\n",
    "            val, a = torch.max(Q, axis=1)\n",
    "            a = a.item()\n",
    "            self.Q.train()\n",
    "        return a\n",
    "\n",
    "    def act_deterministic(self, observation: torch.Tensor):\n",
    "        self.Q.eval()\n",
    "        Q = self.Q(observation)\n",
    "        val, a = torch.max(Q, axis=1)\n",
    "        self.Q.train()\n",
    "        return a.item()"
   ],
   "metadata": {
    "id": "6bLWOSs90KVp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1683571847121,
     "user_tz": 420,
     "elapsed": 813,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the environment"
   ],
   "metadata": {
    "id": "kbjagrA3pXS7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "config = {\n",
    "    'dim_obs': 4,  # Q network input\n",
    "    'dim_action': 2,  # Q network output\n",
    "    'dims_hidden_neurons': (64, 64),  # Q network hidden\n",
    "    'lr': 0.0005,  # learning rate\n",
    "    'C': 60,  # copy steps\n",
    "    'discount': 0.99,  # discount factor\n",
    "    'batch_size': 64,\n",
    "    'replay_buffer_size': 100000,\n",
    "    'eps_min': 0.01,\n",
    "    'eps_max': 1.0,\n",
    "    'eps_len': 4000,\n",
    "    'seed': 1,\n",
    "}\n"
   ],
   "metadata": {
    "id": "zAgJklHgzOmg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1683571851068,
     "user_tz": 420,
     "elapsed": 326,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the DQN agent"
   ],
   "metadata": {
    "id": "kVuWeg79pdk-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dqn = DQN(config)\n",
    "buffer = ReplayBuffer(config)\n",
    "train_writer = SummaryWriter(log_dir='tensorboard/dqn')"
   ],
   "metadata": {
    "id": "MAKp2pKgzYZM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1683571853631,
     "user_tz": 420,
     "elapsed": 4,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start training"
   ],
   "metadata": {
    "id": "T9MJiJNwphUe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "steps = 0  # total number of steps\n",
    "for i_episode in range(500):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    t = 0  # time steps within each episode\n",
    "    ret = 0.  # episodic return\n",
    "    while done is False and truncated is False:\n",
    "        # env.render()  # render to screen, not working for jupyter\n",
    "\n",
    "        obs = torch.tensor(env.state)  # observe the environment state\n",
    "\n",
    "        action = dqn.act_probabilistic(obs[None, :])  # take action\n",
    "\n",
    "        next_obs, reward, done, info,_ = env.step(action)  # environment advance to next step\n",
    "\n",
    "        buffer.append_memory(obs=obs,  # put the transition to memory\n",
    "                             action=torch.from_numpy(np.array([action])),\n",
    "                             reward=torch.from_numpy(np.array([reward])),\n",
    "                             next_obs=torch.from_numpy(next_obs),\n",
    "                             done=done)\n",
    "\n",
    "        dqn.update(buffer)  # agent learn\n",
    "\n",
    "        t += 1\n",
    "        steps += 1\n",
    "        ret += reward  # update episodic return\n",
    "        if done or truncated:\n",
    "            print(\"Episode {} finished after {} timesteps\".format(i_episode, t+1))\n",
    "        train_writer.add_scalar('Performance/episodic_return', ret, i_episode)  # plot\n",
    "\n",
    "env.close()\n",
    "train_writer.close()"
   ],
   "metadata": {
    "id": "0kfeRfmDzRk5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing"
   ],
   "metadata": {
    "id": "7fONEK3AplX3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%tensorboard --logdir='tensorboard/dqn'"
   ],
   "metadata": {
    "id": "ReuJMkvHgcCA"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
