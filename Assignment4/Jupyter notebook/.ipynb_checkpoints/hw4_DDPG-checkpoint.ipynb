{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNcbObRZvC0sirRr9yMALH5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Introduction\n","In homework assignment 3, we will implement the deep deterministic policy gradient (DDPG) algorithm to solve a classic rocket trajectory optimization problem--Lunar Lander v2"],"metadata":{"id":"TmR2wJze-Hx1"}},{"cell_type":"markdown","source":["# Enabling and testing the GPU\n","\n","First, you may need to enable GPUs for this notebook:\n","\n","- Navigate to Editâ†’Notebook Settings\n","- select GPU from the Hardware Accelerator drop-down\n","\n","Next, we'll confirm that we can connect to the GPU with pytorch:"],"metadata":{"id":"ijsToSg1aWq5"}},{"cell_type":"code","source":["import torch\n","import os\n","if torch.cuda.is_available():\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","    device = torch.device('cuda:0')\n","else:\n","    device = torch.device('cpu')\n","print('Found device at: {}'.format(device))"],"metadata":{"id":"iOJLXUtYbGNS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Install gym environment"],"metadata":{"id":"drR9Xee_PjzH"}},{"cell_type":"code","source":["!pip install swig\n","!pip install gymnasium[box2d]"],"metadata":{"id":"rWCQjmWlH7Y9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load tensorboard for visualizing"],"metadata":{"id":"GA5VgUhfPr_B"}},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"-ZV3E-RKIAU-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Import required package"],"metadata":{"id":"6CSA9vTeQFOc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CbM3yFdCFwsA"},"outputs":[],"source":["import torch.nn as nn\n","from typing import Tuple\n","from collections import namedtuple\n","from collections import deque\n","import numpy.random as nr\n","import numpy as np\n","\n","import gymnasium as gym\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import datetime\n","import copy\n","\n","Tensor = torch.DoubleTensor\n","torch.set_default_tensor_type(Tensor)\n","Transitions = namedtuple('Transitions', ['obs', 'action', 'reward', 'next_obs', 'done'])"]},{"cell_type":"markdown","source":["#Replay buffer"],"metadata":{"id":"ayZsQyOKQI7q"}},{"cell_type":"code","source":["class ReplayBuffer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        replay_buffer_size = config['replay_buffer_size']\n","        seed = config['seed']\n","        self.device = config['device']\n","        nr.seed(seed)\n","\n","        self.replay_buffer_size = replay_buffer_size\n","        self.obs = deque([], maxlen=self.replay_buffer_size)\n","        self.action = deque([], maxlen=self.replay_buffer_size)\n","        self.reward = deque([], maxlen=self.replay_buffer_size)\n","        self.next_obs = deque([], maxlen=self.replay_buffer_size)\n","        self.done = deque([], maxlen=self.replay_buffer_size)\n","\n","    def append_memory(self,\n","                      obs,\n","                      action,\n","                      reward,\n","                      next_obs,\n","                      done: bool):\n","        self.obs.append(obs)\n","        self.action.append(action)\n","        self.reward.append(reward)\n","        self.next_obs.append(next_obs)\n","        self.done.append(done)\n","\n","    def sample(self, batch_size):\n","        buffer_size = len(self.obs)\n","\n","        idx = nr.choice(buffer_size,\n","                        size=min(buffer_size, batch_size),\n","                        replace=False)\n","        t = Transitions\n","        t.obs = torch.stack(list(map(self.obs.__getitem__, idx))).to(self.device)\n","        t.action = torch.stack(list(map(self.action.__getitem__, idx))).to(self.device)\n","        t.reward = torch.stack(list(map(self.reward.__getitem__, idx))).to(self.device)\n","        t.next_obs = torch.stack(list(map(self.next_obs.__getitem__, idx))).to(self.device)\n","        t.done = torch.tensor(list(map(self.done.__getitem__, idx)))[:, None].to(self.device)\n","        return t\n","\n","    def clear(self):\n","        self.obs = deque([], maxlen=self.replay_buffer_size)\n","        self.action = deque([], maxlen=self.replay_buffer_size)\n","        self.reward = deque([], maxlen=self.replay_buffer_size)\n","        self.next_obs = deque([], maxlen=self.replay_buffer_size)\n","        self.done = deque([], maxlen=self.replay_buffer_size)"],"metadata":{"id":"WPrR2uBRGQ0P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Actor network"],"metadata":{"id":"mhRz9_6wQMXG"}},{"cell_type":"code","source":["class ActorNet(nn.Module):\n","    def __init__(self,\n","                 dim_obs: int,\n","                 dim_action: int,\n","                 dims_hidden_neurons: Tuple[int] = (64, 64)):\n","        super(ActorNet, self).__init__()\n","        self.n_layers = len(dims_hidden_neurons)\n","        self.dim_action = dim_action\n","\n","        n_neurons = (dim_obs,) + dims_hidden_neurons + (dim_action,)\n","        for i, (dim_in, dim_out) in enumerate(zip(n_neurons[:-2], n_neurons[1:-1])):\n","            layer = nn.Linear(dim_in, dim_out).double()\n","            # nn.Linear: input: (batch_size, n_feature)\n","            #            output: (batch_size, n_output)\n","            torch.nn.init.xavier_uniform_(layer.weight)\n","            torch.nn.init.zeros_(layer.bias)\n","            exec('self.layer{} = layer'.format(i + 1))  # exec(str): execute a short program written in the str\n","\n","        self.output = nn.Linear(n_neurons[-2], n_neurons[-1]).double()\n","        torch.nn.init.xavier_uniform_(self.output.weight)\n","        torch.nn.init.zeros_(self.output.bias)\n","\n","    def forward(self, obs: torch.Tensor):\n","        x = obs\n","        for i in range(self.n_layers):\n","            x = eval('torch.relu(self.layer{}(x))'.format(i + 1))\n","        a = torch.tanh(self.output(x))\n","        return a\n"],"metadata":{"id":"Ef4Kq1P9F8yw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Critic network"],"metadata":{"id":"BtoAgyYZQVbC"}},{"cell_type":"code","source":["class QCriticNet(nn.Module):\n","    def __init__(self,\n","                 dim_obs: int,\n","                 dim_action: int,\n","                 dims_hidden_neurons: Tuple[int] = (64, 64)):\n","        super(QCriticNet, self).__init__()\n","        self.n_layers = len(dims_hidden_neurons)\n","        self.dim_action = dim_action\n","\n","        n_neurons = (dim_obs + dim_action,) + dims_hidden_neurons + (1,)\n","        for i, (dim_in, dim_out) in enumerate(zip(n_neurons[:-2], n_neurons[1:-1])):\n","            layer = nn.Linear(dim_in, dim_out).double()\n","            # nn.Linear: input: (batch_size, n_feature)\n","            #            output: (batch_size, n_output)\n","            torch.nn.init.xavier_uniform_(layer.weight)\n","            torch.nn.init.zeros_(layer.bias)\n","            exec('self.layer{} = layer'.format(i + 1))  # exec(str): execute a short program written in the str\n","\n","        self.output = nn.Linear(n_neurons[-2], n_neurons[-1]).double()\n","        torch.nn.init.xavier_uniform_(self.output.weight)\n","        torch.nn.init.zeros_(self.output.bias)\n","\n","    def forward(self, obs: torch.Tensor, action: torch.Tensor):\n","        x = torch.cat((obs, action), dim=1)\n","        for i in range(self.n_layers):\n","            x = eval('torch.relu(self.layer{}(x))'.format(i + 1))\n","        return self.output(x)\n"],"metadata":{"id":"wStS68fUGAqh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#DDPG agent\n","The base code are given in this section. The updates of the actor and critic networks are missing and are left out for you to fill. You may refer to the DDPG paper https://arxiv.org/pdf/1509.02971.pdf or Spinning up tutorial for DDPG https://spinningup.openai.com/en/latest/algorithms/ddpg.html"],"metadata":{"id":"-EsMwRzpQa_I"}},{"cell_type":"code","source":["class DDPG(nn.Module):\n","    def __init__(self, config):\n","        super(DDPG,self).__init__()\n","        torch.manual_seed(config['seed'])\n","\n","        self.lr_actor = config['lr_actor']  # learning rate\n","        self.lr_critic = config['lr_critic']\n","        self.smooth = config['smooth']  # smoothing coefficient for target net\n","        self.discount = config['discount']  # discount factor\n","        self.batch_size = config['batch_size']  # mini batch size\n","        self.sig = config['sig']  # exploration noise\n","\n","        self.dims_hidden_neurons = config['dims_hidden_neurons']\n","        self.dim_obs = config['dim_obs']\n","        self.dim_action = config['dim_action']\n","\n","        self.device = config['device']\n","\n","        self.actor = ActorNet(dim_obs=self.dim_obs,\n","                              dim_action=self.dim_action,\n","                              dims_hidden_neurons=self.dims_hidden_neurons).to(self.device)\n","        self.Q = QCriticNet(dim_obs=self.dim_obs,\n","                            dim_action=self.dim_action,\n","                            dims_hidden_neurons=self.dims_hidden_neurons).to(self.device)\n","        self.actor_tar = ActorNet(dim_obs=self.dim_obs,\n","                                  dim_action=self.dim_action,\n","                                  dims_hidden_neurons=self.dims_hidden_neurons).to(self.device)\n","        self.Q_tar = QCriticNet(dim_obs=self.dim_obs,\n","                                dim_action=self.dim_action,\n","                                dims_hidden_neurons=self.dims_hidden_neurons).to(self.device)\n","\n","        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n","        self.optimizer_Q = torch.optim.Adam(self.Q.parameters(), lr=self.lr_critic)\n","\n","    def update(self, buffer):\n","        # sample from replay memory\n","        t = buffer.sample(self.batch_size)\n","\n","        # TO DO: Perform the updates for the actor and critic networks\n","\n","\n","    def act_probabilistic(self, obs: torch.Tensor):\n","        self.actor.eval()\n","        exploration_noise = torch.normal(torch.zeros(size=(self.dim_action,)), self.sig).to(self.device)\n","        a = self.actor(obs) + exploration_noise\n","        self.actor.train()\n","        return a\n","\n","    def act_deterministic(self, obs: torch.Tensor):\n","        self.actor.eval()\n","        a = self.actor(obs)\n","        self.actor.train()\n","        return a\n"],"metadata":{"id":"wWjNiWQbF3_I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Create environment"],"metadata":{"id":"OPRE-LSMQgo3"}},{"cell_type":"code","source":["env = gym.make('LunarLanderContinuous-v2')\n","\n","config = {\n","    'dim_obs': 8,\n","    'dim_action': 2,\n","    'dims_hidden_neurons': (400, 200),\n","    'lr_actor': 0.001,\n","    'lr_critic': 0.005,\n","    'smooth': 0.99,\n","    'discount': 0.99,\n","    'sig': 0.01,\n","    'batch_size': 32,\n","    'replay_buffer_size': 20000,\n","    'seed': 1,\n","    'max_episode': 500,\n","    'device':device\n","}\n"],"metadata":{"id":"VSt-brnJGqfv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Create agent"],"metadata":{"id":"-ZBz-XKLQoji"}},{"cell_type":"code","source":["ddpg = DDPG(config).to(device)\n","buffer = ReplayBuffer(config)\n","train_writer = SummaryWriter(log_dir='tensorboard/ddpg')"],"metadata":{"id":"nSFJtfOZG37r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Start training"],"metadata":{"id":"DfGQQi7MQrN-"}},{"cell_type":"code","source":["steps = 0\n","for i_episode in range(config['max_episode']):\n","    obs = env.reset()[0]\n","    done = False\n","    truncated = False\n","    t = 0\n","    ret = 0.\n","    while done is False and truncated is False:\n","        # env.render() \n","\n","        obs_tensor = torch.tensor(obs).type(Tensor).to(device)\n","\n","        action = ddpg.act_probabilistic(obs_tensor[None, :]).detach().cpu().numpy()[0, :]\n","\n","        next_obs, reward, done, truncated,_ = env.step(action)\n","\n","        buffer.append_memory(obs=obs_tensor,\n","                             action=torch.from_numpy(action),\n","                             reward=torch.from_numpy(np.array([reward/10.0])),\n","                             next_obs=torch.from_numpy(next_obs).type(Tensor),\n","                             done=done)\n","\n","        ddpg.update(buffer)\n","\n","        t += 1\n","        steps += 1\n","        ret += reward\n","\n","        obs = copy.deepcopy(next_obs)\n","\n","        if done or truncated:\n","            print(\"Episode {} return {}\".format(i_episode, ret))\n","        train_writer.add_scalar('Performance/episodic_return', ret, i_episode)\n","\n","env.close()\n","train_writer.close()\n"],"metadata":{"id":"NlxydGbvG_0X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Visualizing"],"metadata":{"id":"uH3NVzYtQtvW"}},{"cell_type":"code","source":["%tensorboard --logdir='tensorboard/ddpg'"],"metadata":{"id":"QsNEOgkoH2_S"},"execution_count":null,"outputs":[]}]}