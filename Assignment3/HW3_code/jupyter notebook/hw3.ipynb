{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XYaiuqZ1R34"
   },
   "source": [
    "# Introduction\n",
    "In homework assignment 2, we will implement a basic deep Q learning (DQL) algorithm to solve a classic control problem--CartPole V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb-TSGU816Gk"
   },
   "source": [
    "# Install the gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "abEdBl_00ggY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from gymnasium) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from gymnasium) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in e:\\ucr_projects\\rlproject\\rlenv\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YD3C_WMpC0q"
   },
   "source": [
    "# Load tensorboard for visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683571808819,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "MTv7U2CInzwq"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI4gae3Y2ADK"
   },
   "source": [
    "# Import the required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16082,
     "status": "ok",
     "timestamp": 1683571827632,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "vxIeU-HZwp_x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\UCR_Projects\\RLProject\\RLenv\\lib\\site-packages\\torch\\__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from typing import Tuple\n",
    "from numpy.random import binomial\n",
    "from numpy.random import choice\n",
    "import numpy.random as nr\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Tensor = torch.DoubleTensor\n",
    "torch.set_default_tensor_type(Tensor)\n",
    "Transitions = namedtuple('Transitions', ['obs', 'action', 'reward', 'next_obs', 'done'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVJcJ-IM2L70"
   },
   "source": [
    "# Replay buffer to collect transition tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683571831530,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "m33LVAxM0BBG"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, config):\n",
    "        replay_buffer_size = config['replay_buffer_size']\n",
    "        seed = config['seed']\n",
    "        nr.seed(seed)\n",
    "\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.action = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.reward = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.next_obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.done = deque([], maxlen=self.replay_buffer_size)\n",
    "\n",
    "    def append_memory(self,\n",
    "                      obs,\n",
    "                      action,\n",
    "                      reward,\n",
    "                      next_obs,\n",
    "                      done: bool):\n",
    "        self.obs.append(obs)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.next_obs.append(next_obs)\n",
    "        self.done.append(done)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.obs)\n",
    "\n",
    "        idx = nr.choice(buffer_size,\n",
    "                        size=min(buffer_size, batch_size),\n",
    "                        replace=False)\n",
    "        t = Transitions\n",
    "        t.obs = torch.stack(list(map(self.obs.__getitem__, idx)))\n",
    "        t.action = torch.stack(list(map(self.action.__getitem__, idx)))\n",
    "        t.reward = torch.stack(list(map(self.reward.__getitem__, idx)))\n",
    "        t.next_obs = torch.stack(list(map(self.next_obs.__getitem__, idx)))\n",
    "        t.done = torch.tensor(list(map(self.done.__getitem__, idx)))[:, None]\n",
    "        return t\n",
    "\n",
    "    def clear(self):\n",
    "        self.obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.action = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.reward = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.next_obs = deque([], maxlen=self.replay_buffer_size)\n",
    "        self.done = deque([], maxlen=self.replay_buffer_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8fWBjcA20jD"
   },
   "source": [
    "# Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683571834223,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "3Mo2I5L50pqY"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_obs: int,\n",
    "                 dim_action: int,\n",
    "                 dims_hidden_neurons: Tuple[int] = (64, 64)):\n",
    "        if not isinstance(dim_obs, int):\n",
    "            TypeError('dimension of observation must be int')\n",
    "        if not isinstance(dim_action, int):\n",
    "            TypeError('dimension of action must be int')\n",
    "        if not isinstance(dims_hidden_neurons, tuple):\n",
    "            TypeError('dimensions of hidden neurons must be tuple of int')\n",
    "\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.num_layers = len(dims_hidden_neurons)\n",
    "        self.dim_action = dim_action\n",
    "\n",
    "        n_neurons = (dim_obs, ) + dims_hidden_neurons + (dim_action, )\n",
    "        for i, (dim_in, dim_out) in enumerate(zip(n_neurons[:-2], n_neurons[1:-1])):\n",
    "            layer = nn.Linear(dim_in, dim_out).double()\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            torch.nn.init.zeros_(layer.bias)\n",
    "            exec('self.layer{} = layer'.format(i + 1))\n",
    "\n",
    "        self.output = nn.Linear(n_neurons[-2], n_neurons[-1]).double()\n",
    "        torch.nn.init.xavier_uniform_(self.output.weight)\n",
    "        torch.nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def forward(self, observation: torch.Tensor):\n",
    "        x = observation.double()\n",
    "        for i in range(self.num_layers):\n",
    "            x = eval('torch.tanh(self.layer{}(x))'.format(i + 1))\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIBRWxqp2UYE"
   },
   "source": [
    "# DQN agent\n",
    "The base code are given in this section. The updates of the neural networks are missing and are left out for you to fill. You may refer to the DQN papaer: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1683571847121,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "6bLWOSs90KVp"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, config):\n",
    "\n",
    "        torch.manual_seed(config['seed'])\n",
    "\n",
    "        self.lr = config['lr']  # learning rate\n",
    "        self.C = config['C']  # copy steps\n",
    "        self.eps_len = config['eps_len']  # length of epsilon greedy exploration\n",
    "        self.eps_max = config['eps_max']\n",
    "        self.eps_min = config['eps_min']\n",
    "        self.discount = config['discount']  # discount factor\n",
    "        self.batch_size = config['batch_size']  # mini batch size\n",
    "\n",
    "        self.dims_hidden_neurons = config['dims_hidden_neurons']\n",
    "        self.dim_obs = config['dim_obs']\n",
    "        self.dim_action = config['dim_action']\n",
    "\n",
    "        self.Q = QNetwork(dim_obs=self.dim_obs,\n",
    "                          dim_action=self.dim_action,\n",
    "                          dims_hidden_neurons=self.dims_hidden_neurons)\n",
    "        self.Q_tar = QNetwork(dim_obs=self.dim_obs,\n",
    "                              dim_action=self.dim_action,\n",
    "                              dims_hidden_neurons=self.dims_hidden_neurons)\n",
    "\n",
    "        self.optimizer_Q = torch.optim.Adam(self.Q.parameters(), lr=self.lr)\n",
    "        self.training_step = 0\n",
    "\n",
    "    def update(self, buffer):\n",
    "        t = buffer.sample(self.batch_size)\n",
    "\n",
    "        s = t.obs\n",
    "        a = t.action\n",
    "        r = t.reward\n",
    "        sp = t.next_obs\n",
    "        done = t.done\n",
    "\n",
    "        self.training_step += 1\n",
    "\n",
    "        # TODO: perform a single Q network update step. Also update the target Q network every C Q network update steps\n",
    "        # Q network update step\n",
    "        # self.Q.train()\n",
    "        # self.Q_tar.eval()\n",
    "\n",
    "        a = a.long()\n",
    "        # Calculate Q values for current states\n",
    "        Q_current_value = self.Q(s).gather(1, a)\n",
    "    \n",
    "        # Compute target Q values\n",
    "         \n",
    "        # Q_target_value = r + self.discount * torch.max(self.Q_tar(sp), dim=1)[0]\n",
    "        Q_target_value = r + self.discount * self.Q_tar(sp).detach().max(1)[0].unsqueeze(1) * (1 - done.float())\n",
    "\n",
    "        # print(a)\n",
    "        # print(Q_current_value)\n",
    "        # print(self.Q_tar(sp))\n",
    "        # print(Q_target_value)\n",
    "        # print()\n",
    "        # print()\n",
    "        # # Calculate loss\n",
    "        loss = F.mse_loss(Q_current_value, Q_target_value)\n",
    "    \n",
    "        # Optimize the Q network\n",
    "        # loss.backward()\n",
    "        self.optimizer_Q.zero_grad()  # clear gradients since PyTorch accumulates them\n",
    "        loss.backward()\n",
    "        self.optimizer_Q.step()\n",
    "    \n",
    "        # # Update target Q network every C steps\n",
    "        if self.training_step % self.C == 0:\n",
    "            self.Q_tar.load_state_dict(self.Q.state_dict())\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "\n",
    "    def act_probabilistic(self, observation: torch.Tensor):\n",
    "        # epsilon greedy:\n",
    "        first_term = self.eps_max * (self.eps_len - self.training_step) / self.eps_len\n",
    "        eps = max(first_term, self.eps_min)\n",
    "\n",
    "        explore = binomial(1, eps)\n",
    "\n",
    "        if explore == 1:\n",
    "            a = choice(self.dim_action)\n",
    "        else:\n",
    "            self.Q.eval()\n",
    "            Q = self.Q(observation)\n",
    "            val, a = torch.max(Q, axis=1)\n",
    "            a = a.item()\n",
    "            self.Q.train()\n",
    "        return a\n",
    "\n",
    "    def act_deterministic(self, observation: torch.Tensor):\n",
    "        self.Q.eval()\n",
    "        Q = self.Q(observation)\n",
    "        val, a = torch.max(Q, axis=1)\n",
    "        self.Q.train()\n",
    "        return a.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbjagrA3pXS7"
   },
   "source": [
    "# Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1683571851068,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "zAgJklHgzOmg"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "config = {\n",
    "    'dim_obs': 4,  # Q network input\n",
    "    'dim_action': 2,  # Q network output\n",
    "    'dims_hidden_neurons': (64, 64),  # Q network hidden\n",
    "    'lr': 0.0005,  # learning rate\n",
    "    'C': 60,  # copy steps\n",
    "    'discount': 0.99,  # discount factor\n",
    "    'batch_size': 64,\n",
    "    'replay_buffer_size': 100000,\n",
    "    'eps_min': 0.01,\n",
    "    'eps_max': 1.0,\n",
    "    'eps_len': 4000,\n",
    "    'seed': 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVuWeg79pdk-"
   },
   "source": [
    "# Create the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683571853631,
     "user": {
      "displayName": "Jingtao Qin",
      "userId": "03406000436496256014"
     },
     "user_tz": 420
    },
    "id": "MAKp2pKgzYZM"
   },
   "outputs": [],
   "source": [
    "dqn = DQN(config)\n",
    "buffer = ReplayBuffer(config)\n",
    "train_writer = SummaryWriter(log_dir='tensorboard/dqn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9MJiJNwphUe"
   },
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "0kfeRfmDzRk5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()  \u001b[38;5;66;03m# render to screen, not working for jupyter\u001b[39;00m\n\u001b[0;32m     12\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(env\u001b[38;5;241m.\u001b[39mstate)  \u001b[38;5;66;03m# observe the environment state\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_probabilistic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# take action\u001b[39;00m\n\u001b[0;32m     16\u001b[0m next_obs, reward, done, info,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# environment advance to next step\u001b[39;00m\n\u001b[0;32m     18\u001b[0m buffer\u001b[38;5;241m.\u001b[39mappend_memory(obs\u001b[38;5;241m=\u001b[39mobs,  \u001b[38;5;66;03m# put the transition to memory\u001b[39;00m\n\u001b[0;32m     19\u001b[0m                      action\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray([action])),\n\u001b[0;32m     20\u001b[0m                      reward\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray([reward])),\n\u001b[0;32m     21\u001b[0m                      next_obs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfrom_numpy(next_obs),\n\u001b[0;32m     22\u001b[0m                      done\u001b[38;5;241m=\u001b[39mdone)\n",
      "Cell \u001b[1;32mIn[65], line 86\u001b[0m, in \u001b[0;36mDQN.act_probabilistic\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 86\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     val, a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(Q, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     88\u001b[0m     a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mE:\\UCR_Projects\\RLProject\\RLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\UCR_Projects\\RLProject\\RLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch.tanh(self.layer\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m(x))\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "File \u001b[1;32mE:\\UCR_Projects\\RLProject\\RLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\UCR_Projects\\RLProject\\RLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\UCR_Projects\\RLProject\\RLenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 0  # total number of steps\n",
    "\n",
    "for i_episode in range(500):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    t = 0  # time steps within each episode\n",
    "    ret = 0.  # episodic return\n",
    "    while done is False and truncated is False:\n",
    "        env.render()  # render to screen, not working for jupyter\n",
    "\n",
    "        obs = torch.tensor(env.state)  # observe the environment state\n",
    "\n",
    "        action = dqn.act_probabilistic(obs[None, :])  # take action\n",
    "\n",
    "        next_obs, reward, done, info,_ = env.step(action)  # environment advance to next step\n",
    "\n",
    "        buffer.append_memory(obs=obs,  # put the transition to memory\n",
    "                             action=torch.from_numpy(np.array([action])),\n",
    "                             reward=torch.from_numpy(np.array([reward])),\n",
    "                             next_obs=torch.from_numpy(next_obs),\n",
    "                             done=done)\n",
    "\n",
    "        dqn.update(buffer)  # agent learn\n",
    "\n",
    "        t += 1\n",
    "        steps += 1\n",
    "        ret += reward  # update episodic return\n",
    "        if done or truncated:\n",
    "            print(\"Episode {} finished after {} timesteps\".format(i_episode, t+1))\n",
    "        train_writer.add_scalar('Performance/episodic_return', ret, i_episode)  # plot\n",
    "\n",
    "env.close()\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fONEK3AplX3"
   },
   "source": [
    "# Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReuJMkvHgcCA"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir='tensorboard/dqn'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNWQ2OaUIKGRb8LDhnzck9Z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
